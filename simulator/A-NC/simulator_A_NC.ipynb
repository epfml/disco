{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simulator_A_NC.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hr7nxROzvvA"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from utils import *\n",
        "from A_NC import *"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "ihcwsh3z4dKK",
        "outputId": "13f8ece1-fe1d-4fd5-91f0-344a8a29c1da"
      },
      "source": [
        "# IID case: all the clients have images of all the classes\r\n",
        "# Grid graph topology: each client is connected to exactly 4 neighbours\r\n",
        "\r\n",
        "# Hyperparameters\r\n",
        "\r\n",
        "num_clients = 100\r\n",
        "num_rounds = 10\r\n",
        "epochs = 1\r\n",
        "batch_size = 32\r\n",
        "\r\n",
        "# Communication matrix\r\n",
        "\r\n",
        "comm_matrix = create_mixing_matrix('grid', num_clients)\r\n",
        "\r\n",
        "# Creating decentralized datasets\r\n",
        "\r\n",
        "train_loader, test_loader = load_data(batch_size, num_clients)\r\n",
        "\r\n",
        "# Instantiate models and optimizers and run decentralized training\r\n",
        "\r\n",
        "global_model, client_models, accs = run(train_loader, test_loader, comm_matrix, num_rounds, epochs, num_clients)\r\n",
        "\r\n",
        "cons = consensus(global_model, client_models)\r\n",
        "print(cons)\r\n",
        "\r\n",
        "axes = plt.gca()\r\n",
        "axes.set_ylim([0,1])\r\n",
        "plt.plot(range(num_rounds), accs)\r\n",
        "plt.show()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0-th round\n",
            "average train loss 2.29 | test loss 2.28 | test acc: 0.130\n",
            "1-th round\n",
            "average train loss 2.19 | test loss 2.17 | test acc: 0.521\n",
            "2-th round\n",
            "average train loss 1.52 | test loss 1.17 | test acc: 0.742\n",
            "3-th round\n",
            "average train loss 0.991 | test loss 0.69 | test acc: 0.818\n",
            "4-th round\n",
            "average train loss 0.698 | test loss 0.492 | test acc: 0.870\n",
            "5-th round\n",
            "average train loss 0.586 | test loss 0.387 | test acc: 0.898\n",
            "6-th round\n",
            "average train loss 0.463 | test loss 0.329 | test acc: 0.909\n",
            "7-th round\n",
            "average train loss 0.455 | test loss 0.292 | test acc: 0.920\n",
            "8-th round\n",
            "average train loss 0.378 | test loss 0.255 | test acc: 0.926\n",
            "9-th round\n",
            "average train loss 0.317 | test loss 0.235 | test acc: 0.931\n",
            "[0.021848, 0.02027, 0.020793, 0.018671, 0.020131, 0.018555, 0.019607, 0.019629, 0.020796, 0.018636, 0.020631, 0.019845, 0.020866, 0.018653, 0.017959, 0.018219, 0.019684, 0.018842, 0.020298, 0.016769, 0.019208, 0.017306, 0.018531, 0.018055, 0.017198, 0.018142, 0.017858, 0.017942, 0.02019, 0.018018, 0.021136, 0.01957, 0.018237, 0.017389, 0.017486, 0.017913, 0.017188, 0.017734, 0.020838, 0.019176, 0.02194, 0.021649, 0.019146, 0.018927, 0.017581, 0.017413, 0.017739, 0.018562, 0.020367, 0.018132, 0.021352, 0.021966, 0.019461, 0.017723, 0.017211, 0.018103, 0.018002, 0.01822, 0.018582, 0.017116, 0.019956, 0.019907, 0.018803, 0.017102, 0.01837, 0.018625, 0.018561, 0.018818, 0.018373, 0.016964, 0.019019, 0.017336, 0.017952, 0.017969, 0.01841, 0.018739, 0.019404, 0.017455, 0.017686, 0.016634, 0.019278, 0.01634, 0.017075, 0.01747, 0.017694, 0.01909, 0.018344, 0.016833, 0.017042, 0.016195, 0.019325, 0.016513, 0.016545, 0.017548, 0.017123, 0.016354, 0.015777, 0.016168, 0.017123, 0.015983]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcU6m-FvelHv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b476eea-5058-449b-f299-87f9c879235f"
      },
      "source": [
        "# IID case: all the clients have images of all the classes\n",
        "# Grid graph topology: each client is connected to exactly 4 neighbours\n",
        "\n",
        "# Hyperparameters\n",
        "\n",
        "num_clients = 100\n",
        "num_rounds = 10\n",
        "num_runs = 10\n",
        "epochs = 1\n",
        "batch_size = 32\n",
        "\n",
        "# Communication matrix\n",
        "\n",
        "comm_matrix = create_mixing_matrix('grid', num_clients)\n",
        "\n",
        "# Creating decentralized datasets\n",
        "\n",
        "train_loader, test_loader = load_data(batch_size, num_clients)\n",
        "\n",
        "# Instantiate models and optimizers and run decentralized training\n",
        "\n",
        "global_model_list = []\n",
        "client_models_list = []\n",
        "\n",
        "for p in range(num_runs):\n",
        "    print(\"%d-th run\" % p)\n",
        "    _, client_models, _ = run(train_loader, test_loader, comm_matrix, num_rounds, epochs, num_clients)\n",
        "    #global_model_list.append(list(global_model.state_dict().values()))\n",
        "    client_models_list.append([list(client.state_dict().values()) for client in client_models])\n",
        "\n",
        "avg_global_model = Net().cuda()\n",
        "\n",
        "avg_client_models = [Net().cuda() for _ in range(num_clients)]\n",
        "avg_client_models_list = np.array(client_models_list).mean(0)\n",
        "\n",
        "for i in range (num_clients):\n",
        "    client = avg_client_models[i]\n",
        "    client_dict = client.state_dict()\n",
        "    j = 0\n",
        "    for k in client_dict.keys():\n",
        "        client_dict[k] = avg_client_models_list[i][j]\n",
        "        j += 1\n",
        "    client.load_state_dict(client_dict)\n",
        " \n",
        "average_models(avg_global_model, avg_client_models)\n",
        "\n",
        "test_loss, acc = evaluate(avg_global_model, test_loader)\n",
        "\n",
        "print('final round')\n",
        "print('test loss %0.3g | test acc: %0.3f' % (test_loss, acc))\n",
        "\n",
        "cons = consensus(avg_global_model, avg_client_models)\n",
        "print(\"Consensus values: \", cons)\n",
        "print(\"Mean consensus value: \", np.array(cons).mean())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0-th run\n",
            "0-th round\n",
            "average train loss 2.25 | test loss 2.24 | test acc: 0.479\n",
            "1-th round\n",
            "average train loss 1.68 | test loss 1.41 | test acc: 0.724\n",
            "2-th round\n",
            "average train loss 1.05 | test loss 0.774 | test acc: 0.800\n",
            "3-th round\n",
            "average train loss 0.749 | test loss 0.507 | test acc: 0.866\n",
            "4-th round\n",
            "average train loss 0.657 | test loss 0.401 | test acc: 0.894\n",
            "5-th round\n",
            "average train loss 0.473 | test loss 0.329 | test acc: 0.909\n",
            "6-th round\n",
            "average train loss 0.444 | test loss 0.284 | test acc: 0.919\n",
            "7-th round\n",
            "average train loss 0.395 | test loss 0.256 | test acc: 0.926\n",
            "8-th round\n",
            "average train loss 0.347 | test loss 0.229 | test acc: 0.934\n",
            "9-th round\n",
            "average train loss 0.292 | test loss 0.211 | test acc: 0.938\n",
            "1-th run\n",
            "0-th round\n",
            "average train loss 2.25 | test loss 2.25 | test acc: 0.444\n",
            "1-th round\n",
            "average train loss 1.73 | test loss 1.45 | test acc: 0.734\n",
            "2-th round\n",
            "average train loss 1.09 | test loss 0.764 | test acc: 0.821\n",
            "3-th round\n",
            "average train loss 0.756 | test loss 0.533 | test acc: 0.856\n",
            "4-th round\n",
            "average train loss 0.61 | test loss 0.413 | test acc: 0.888\n",
            "5-th round\n",
            "average train loss 0.505 | test loss 0.336 | test acc: 0.909\n",
            "6-th round\n",
            "average train loss 0.418 | test loss 0.296 | test acc: 0.916\n",
            "7-th round\n",
            "average train loss 0.373 | test loss 0.262 | test acc: 0.927\n",
            "8-th round\n",
            "average train loss 0.366 | test loss 0.236 | test acc: 0.931\n",
            "9-th round\n",
            "average train loss 0.286 | test loss 0.215 | test acc: 0.936\n",
            "2-th run\n",
            "0-th round\n",
            "average train loss 2.27 | test loss 2.27 | test acc: 0.339\n",
            "1-th round\n",
            "average train loss 1.96 | test loss 1.87 | test acc: 0.702\n",
            "2-th round\n",
            "average train loss 1.22 | test loss 0.839 | test acc: 0.799\n",
            "3-th round\n",
            "average train loss 0.778 | test loss 0.564 | test acc: 0.839\n",
            "4-th round\n",
            "average train loss 0.612 | test loss 0.43 | test acc: 0.881\n",
            "5-th round\n",
            "average train loss 0.491 | test loss 0.347 | test acc: 0.905\n",
            "6-th round\n",
            "average train loss 0.43 | test loss 0.297 | test acc: 0.915\n",
            "7-th round\n",
            "average train loss 0.395 | test loss 0.264 | test acc: 0.923\n",
            "8-th round\n",
            "average train loss 0.363 | test loss 0.237 | test acc: 0.931\n",
            "9-th round\n",
            "average train loss 0.312 | test loss 0.215 | test acc: 0.936\n",
            "3-th run\n",
            "0-th round\n",
            "average train loss 2.25 | test loss 2.24 | test acc: 0.388\n",
            "1-th round\n",
            "average train loss 1.76 | test loss 1.45 | test acc: 0.755\n",
            "2-th round\n",
            "average train loss 1.07 | test loss 0.728 | test acc: 0.823\n",
            "3-th round\n",
            "average train loss 0.769 | test loss 0.498 | test acc: 0.866\n",
            "4-th round\n",
            "average train loss 0.603 | test loss 0.376 | test acc: 0.899\n",
            "5-th round\n",
            "average train loss 0.425 | test loss 0.315 | test acc: 0.911\n",
            "6-th round\n",
            "average train loss 0.414 | test loss 0.274 | test acc: 0.921\n",
            "7-th round\n",
            "average train loss 0.359 | test loss 0.243 | test acc: 0.929\n",
            "8-th round\n",
            "average train loss 0.32 | test loss 0.224 | test acc: 0.933\n",
            "9-th round\n",
            "average train loss 0.311 | test loss 0.203 | test acc: 0.939\n",
            "4-th run\n",
            "0-th round\n",
            "average train loss 2.26 | test loss 2.25 | test acc: 0.310\n",
            "1-th round\n",
            "average train loss 1.87 | test loss 1.71 | test acc: 0.679\n",
            "2-th round\n",
            "average train loss 1.17 | test loss 0.813 | test acc: 0.800\n",
            "3-th round\n",
            "average train loss 0.815 | test loss 0.547 | test acc: 0.853\n",
            "4-th round\n",
            "average train loss 0.622 | test loss 0.418 | test acc: 0.889\n",
            "5-th round\n",
            "average train loss 0.525 | test loss 0.345 | test acc: 0.905\n",
            "6-th round\n",
            "average train loss 0.431 | test loss 0.298 | test acc: 0.916\n",
            "7-th round\n",
            "average train loss 0.376 | test loss 0.263 | test acc: 0.924\n",
            "8-th round\n",
            "average train loss 0.351 | test loss 0.237 | test acc: 0.930\n",
            "9-th round\n",
            "average train loss 0.325 | test loss 0.217 | test acc: 0.937\n",
            "5-th run\n",
            "0-th round\n",
            "average train loss 2.29 | test loss 2.29 | test acc: 0.114\n",
            "1-th round\n",
            "average train loss 2.27 | test loss 2.27 | test acc: 0.190\n",
            "2-th round\n",
            "average train loss 2.18 | test loss 2.16 | test acc: 0.580\n",
            "3-th round\n",
            "average train loss 1.57 | test loss 1.23 | test acc: 0.764\n",
            "4-th round\n",
            "average train loss 0.949 | test loss 0.67 | test acc: 0.825\n",
            "5-th round\n",
            "average train loss 0.711 | test loss 0.483 | test acc: 0.871\n",
            "6-th round\n",
            "average train loss 0.59 | test loss 0.387 | test acc: 0.894\n",
            "7-th round\n",
            "average train loss 0.497 | test loss 0.331 | test acc: 0.908\n",
            "8-th round\n",
            "average train loss 0.4 | test loss 0.284 | test acc: 0.919\n",
            "9-th round\n",
            "average train loss 0.367 | test loss 0.25 | test acc: 0.928\n",
            "6-th run\n",
            "0-th round\n",
            "average train loss 2.29 | test loss 2.29 | test acc: 0.306\n",
            "1-th round\n",
            "average train loss 2.24 | test loss 2.23 | test acc: 0.405\n",
            "2-th round\n",
            "average train loss 1.87 | test loss 1.75 | test acc: 0.614\n",
            "3-th round\n",
            "average train loss 1.15 | test loss 0.818 | test acc: 0.798\n",
            "4-th round\n",
            "average train loss 0.763 | test loss 0.52 | test acc: 0.863\n",
            "5-th round\n",
            "average train loss 0.597 | test loss 0.408 | test acc: 0.886\n",
            "6-th round\n",
            "average train loss 0.506 | test loss 0.332 | test acc: 0.908\n",
            "7-th round\n",
            "average train loss 0.426 | test loss 0.288 | test acc: 0.920\n",
            "8-th round\n",
            "average train loss 0.405 | test loss 0.254 | test acc: 0.928\n",
            "9-th round\n",
            "average train loss 0.332 | test loss 0.23 | test acc: 0.934\n",
            "7-th run\n",
            "0-th round\n",
            "average train loss 2.28 | test loss 2.27 | test acc: 0.164\n",
            "1-th round\n",
            "average train loss 2.09 | test loss 2.04 | test acc: 0.629\n",
            "2-th round\n",
            "average train loss 1.33 | test loss 0.987 | test acc: 0.763\n",
            "3-th round\n",
            "average train loss 0.891 | test loss 0.58 | test acc: 0.841\n",
            "4-th round\n",
            "average train loss 0.644 | test loss 0.426 | test acc: 0.885\n",
            "5-th round\n",
            "average train loss 0.519 | test loss 0.35 | test acc: 0.904\n",
            "6-th round\n",
            "average train loss 0.469 | test loss 0.3 | test acc: 0.916\n",
            "7-th round\n",
            "average train loss 0.368 | test loss 0.265 | test acc: 0.922\n",
            "8-th round\n",
            "average train loss 0.349 | test loss 0.24 | test acc: 0.928\n",
            "9-th round\n",
            "average train loss 0.3 | test loss 0.218 | test acc: 0.935\n",
            "8-th run\n",
            "0-th round\n",
            "average train loss 2.25 | test loss 2.23 | test acc: 0.467\n",
            "1-th round\n",
            "average train loss 1.71 | test loss 1.36 | test acc: 0.726\n",
            "2-th round\n",
            "average train loss 1.04 | test loss 0.744 | test acc: 0.797\n",
            "3-th round\n",
            "average train loss 0.795 | test loss 0.51 | test acc: 0.866\n",
            "4-th round\n",
            "average train loss 0.609 | test loss 0.417 | test acc: 0.886\n",
            "5-th round\n",
            "average train loss 0.49 | test loss 0.33 | test acc: 0.910\n",
            "6-th round\n",
            "average train loss 0.427 | test loss 0.285 | test acc: 0.919\n",
            "7-th round\n",
            "average train loss 0.378 | test loss 0.255 | test acc: 0.927\n",
            "8-th round\n",
            "average train loss 0.315 | test loss 0.23 | test acc: 0.932\n",
            "9-th round\n",
            "average train loss 0.33 | test loss 0.212 | test acc: 0.938\n",
            "9-th run\n",
            "0-th round\n",
            "average train loss 2.26 | test loss 2.25 | test acc: 0.361\n",
            "1-th round\n",
            "average train loss 1.81 | test loss 1.62 | test acc: 0.698\n",
            "2-th round\n",
            "average train loss 1.07 | test loss 0.752 | test acc: 0.814\n",
            "3-th round\n",
            "average train loss 0.809 | test loss 0.544 | test acc: 0.848\n",
            "4-th round\n",
            "average train loss 0.585 | test loss 0.399 | test acc: 0.892\n",
            "5-th round\n",
            "average train loss 0.504 | test loss 0.33 | test acc: 0.910\n",
            "6-th round\n",
            "average train loss 0.432 | test loss 0.284 | test acc: 0.922\n",
            "7-th round\n",
            "average train loss 0.369 | test loss 0.254 | test acc: 0.927\n",
            "8-th round\n",
            "average train loss 0.303 | test loss 0.226 | test acc: 0.933\n",
            "9-th round\n",
            "average train loss 0.31 | test loss 0.207 | test acc: 0.939\n",
            "final round\n",
            "test loss 2.3 | test acc: 0.168\n",
            "Consensus values:  [0.002091, 0.001975, 0.002075, 0.002177, 0.002153, 0.00205, 0.002112, 0.002005, 0.002091, 0.002074, 0.002214, 0.001868, 0.00195, 0.002053, 0.001985, 0.001955, 0.002019, 0.001899, 0.001987, 0.001984, 0.002259, 0.002084, 0.002192, 0.002069, 0.002039, 0.002031, 0.002006, 0.001818, 0.001776, 0.001849, 0.002134, 0.002085, 0.002011, 0.002013, 0.002061, 0.002042, 0.002022, 0.00192, 0.001886, 0.001951, 0.002378, 0.002033, 0.002045, 0.002105, 0.002114, 0.002097, 0.001941, 0.001897, 0.002015, 0.002026, 0.002353, 0.002039, 0.002016, 0.002005, 0.001935, 0.00185, 0.001938, 0.001979, 0.002046, 0.002034, 0.002134, 0.001973, 0.001952, 0.001937, 0.001908, 0.001999, 0.002012, 0.001984, 0.002044, 0.002001, 0.002195, 0.002049, 0.002007, 0.001917, 0.001947, 0.002048, 0.002036, 0.001967, 0.002154, 0.002012, 0.002006, 0.002059, 0.002123, 0.002135, 0.002112, 0.002011, 0.001818, 0.001905, 0.002044, 0.001912, 0.001877, 0.001802, 0.00193, 0.001965, 0.001883, 0.001786, 0.00172, 0.0017, 0.001842, 0.001757]\n",
            "Mean consensus value:  0.00200474\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}